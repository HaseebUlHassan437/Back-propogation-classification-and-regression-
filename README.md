# Backpropagation: Classification, Regression, and RAG using LangChain

Welcome to this repository! It contains three detailed Jupyter (Colab) notebooks covering:

1. **Backpropagation for Classification**
2. **Backpropagation for Regression**
3. **RAG (Retrieval-Augmented Generation) Implementation using LangChain**

---

## üìÅ Repository Structure

| File | Description |
| :--- | :--- |
| [`backpropagation_classification.ipynb`](https://github.com/HaseebUlHassan437/Back-propogation-classification-and-regression-/blob/main/backpropagation_classification.ipynb) | Implementation of a Neural Network for Classification using backpropagation from scratch (without using high-level libraries like Keras or TensorFlow). |
| [`backpropagation_regression.ipynb`](https://github.com/HaseebUlHassan437/Back-propogation-classification-and-regression-/blob/main/backpropagation_regression.ipynb) | Implementation of a Neural Network for Regression tasks using backpropagation, demonstrating how the model learns continuous values. |
| [`Rag_using_langchain(By_YOUTUBE_id).ipynb`](https://github.com/HaseebUlHassan437/Back-propogation-classification-and-regression-/blob/main/Rag_using_langchain(By_YOUTUBE_id).ipynb) | Building a simple RAG (Retrieval-Augmented Generation) system using **LangChain** library based on YouTube video IDs. |

---

## üöÄ Project Highlights

### 1. Backpropagation Classification
- A neural network model is designed from scratch.
- Supports multi-class classification tasks.
- Includes manual implementation of forward pass, backward pass (error propagation), and weight updates.

### 2. Backpropagation Regression
- Focuses on predicting continuous outputs.
- Shows how loss functions like Mean Squared Error (MSE) are minimized through backpropagation.
- End-to-end workflow of training and testing a regression model without high-level frameworks.

### 3. RAG using LangChain
- Retrieval-Augmented Generation (RAG) approach using **LangChain**.
- Retrieves information based on YouTube video transcripts and performs intelligent Q&A.
- Introduces basic concepts of connecting external data sources with LLMs for better responses.

---

## üìö Requirements

- Python 3.7+
- Jupyter Notebook / Google Colab
- Libraries:
  - `numpy`
  - `matplotlib`
  - `sklearn`
  - `torch` *(optional for improvements)*
  - `langchain`
  - `openai` *(for RAG if LLM API key is needed)*

Install required libraries using:
```bash
pip install numpy matplotlib scikit-learn langchain openai
```

---

## üìå How to Run

1. **Clone the repository:**
   ```bash
   git clone https://github.com/HaseebUlHassan437/Back-propogation-classification-and-regression-.git
   cd Back-propogation-classification-and-regression-
   ```

2. **Open any notebook in Google Colab**  
   (Recommended for easy execution without setup).

3. **Follow the instructions inside each notebook.**

---

## ‚ú® Future Improvements
- Extend classification and regression networks to handle deeper architectures.
- Integrate HuggingFace Transformers for more powerful RAG systems.
- Add hyperparameter tuning features (learning rate, batch size, etc.)

---

## üßë‚Äçüíª Author

- **Haseeb Ul Hassan**  
  GitHub: [@HaseebUlHassan437](https://github.com/HaseebUlHassan437)

